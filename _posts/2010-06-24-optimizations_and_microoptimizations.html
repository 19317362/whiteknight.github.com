---
layout: bloggerpost
title: Optimizations and Microoptimizations
---

Consider the case where we're running a particular Parrot program that creates one million PMCs, for various purposes. We find that allocating and managing these PMCs, including pressure on the GC to periodically evaluate their liveliness and collect/recycle old ones, is quite expensive. Directly because of this expense the program takes significantly longer to run than it would otherwise need to. What can we do about it?<br /><br />If we look at <a href="http://en.wikipedia.org/wiki/Big_O_Notation">Big O notation</a>, we can find an asymptotic boundary for how expensive an operation is. Big O notation says that our function <i>f(n)</i> satisfies this relationship:<br /><br /><i>f(n) â†’ O(g(n))</i><br /><i>f(n) < cg(n) + d</i><br /><br /><i>g(n)</i> is our boundary function, which shows the rate of growth of the algorithm's expense as the input set size <i>n</i> increases. An expense of <i>n </i>means we need to touch every element a constant number of times. <i>n<sup>2</sup></i> is essentially a nested loop where we touch every element for every element a constant number of times. So on and so forth. <i>c</i> is a cost coefficient to say how expensive the operation is for each iteration, and <i>d</i> is some constant value representing the overhead of the algorithm; setup and cleanup costs usually.<br /><br />Keep in mind that these are asymptotic relationships. For small datasets, if the coefficients are tuned correctly, an <i>O(n<sup>2</sup>)</i> can actually be less expensive than <i>O(1)</i> in some benchmarks. As the data set gets larger, however, the rate of growth of the former eclipses the later and begins to eat up more and more computational time and resources. <br /><br />In the contexts of Parrot's memory systems <i>d</i> is the setup time of the GC and the memory system. This is a fixed constant amount of time and is generally negligible. Improving the startup time of the memory system will improve startup time for Parrot as a whole, but it turns out not to be a particularly expensive part of the whole operation.<br /><br />We have one million PMCs that are getting used in our application. Allocation tends to be constant-time and relatively inexpensive. GC mark and sweep are both about <i>O(n)</i> with various coefficients. For mark, <i>n</i> is the number of traceable pointers to follow and <i>c</i> is based on the relative cost of marking a PMC and attempting to mark a PMC which has already been marked, and the percentage of each that we perform while marking. In sweep, we currently iterate over the entire PMC pool, which is <i>O(n)</i>.<br /><br />For an individual allocation, the complexity is <i>O(1)</i>. Over the course of the application the total cost is <i>O(n)</i>, where <i>n</i> is the number of PMCs we allocate. There isn't a lot of performance to squeeze out here.<br /><br />For mark the cost of each individual mark is typically a check of a flag, setting the flag, and then iterating over all children pointers. There isn't a lot we can do do decrease the per-pointer cost of mark, but using a scheme like generational GC we can decrease <i>n</i> so we attempt to mark fewer pointers.<br /><br />For sweep the cost is related to the total number size of the PMC pool which has been allocated from the OS. Sweep tends to be implemented as a tight loop over an array of PMCs in memory, and has relatively low coefficients as well. Again, the big performance gain here is to find a way to decrease <i>n</i> and find a way to sweep over a smaller set of PMCs.<br /><br />There's a difference to be made between <b>Algorithmic optimizations</b>, and <b>micro-optimizations</b>. In the former case, we find a better algorithm to cut our workload and decrease those pesky <i>n</i>s. In the later case we start using all sorts of dark magic (like passing -O3 to our compiler) to decrease the per-iteration cost of the program. This can mean <a href="http://www.friday.com/bbum/2009/12/18/objc_msgsend-part-1-the-road-map/">decreasing the number of machine code operations</a> used, limiting the number of cache misses and page faults, being <a href="http://wknight8111.blogspot.com/2009/10/parrottheory-microprocessor-design.html">more friendly to our branch predictor</a>, etc.<br /><br />When optimizing, we need to always ask ourselves two questions:<br /><ol><li>How can I decrease the number of iterations</li><li>How can I decrease the cost per iteration</li></ol>In most cases we need to ask the first question first, fix the shape of the algorithm first, tweak the code second. Here are some examples of optimizations we can apply to Parrot, divided by subsystem:<br /><ul><li><b>PCC</b>: We can inline simple functions to decrease the number of calls. Likewise, we can convert recursive tailcalls into loops (IMCC does some of this already) to decrease calls there. We can add a built-in <a href="http://en.wikipedia.org/wiki/Memoization">memozation</a> system for subs marked as being suitable for it, and return values from a hash instead of calling a function again with arguments that it has previously been called with. We can also do <a href="http://en.wikipedia.org/wiki/Inline_caching">PIC or other call-side caching</a> to help speed up method lookup.</li><li><b>GC</b>: As I mentioned above, we can switch to a generational GC to <a href="http://wknight8111.blogspot.com/2009/08/lazy-gc-and-fixed-size-allocation.html">decrease the number of PMCs we need to allocate</a>. I think we can also move towards chromatic's sweep-free system to cut sweep times down dramatically too. </li></ul>